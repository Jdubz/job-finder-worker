# =============================================================================
# LiteLLM Proxy Configuration — Job Finder
# =============================================================================
#
# Unified AI gateway providing OpenAI-compatible API for all services.
# See: https://docs.litellm.ai/docs/proxy/configs
#
# MODEL ROUTING:
# ──────────────
#   claude-document  → Claude Sonnet (via API key)
#   gemini-general   → Gemini 2.5 Flash (via API key)
#   local-extract    → Ollama Llama 3.1 8B (local, no cost)
#
# FALLBACK CHAINS:
# ────────────────
#   claude-document  → gemini-general  (if Claude unavailable)
#   local-extract    → gemini-general → claude-document  (if Ollama down)
#
# =============================================================================

model_list:
  # ── Document generation ─────────────────────────────────────────────────────
  # Claude via standard API key
  - model_name: claude-document
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

  # ── General-purpose fallback ────────────────────────────────────────────────
  # Gemini Flash — fast, cheap, good for extraction and analysis
  - model_name: gemini-general
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY

  # ── Local extraction/analysis ───────────────────────────────────────────────
  # Ollama Llama 3.1 8B — zero cost, runs on local GPU via Docker
  - model_name: local-extract
    litellm_params:
      model: openai/llama3.1:8b
      api_base: http://ollama:11434/v1
      api_key: none

litellm_settings:
  num_retries: 2
  request_timeout: 120
  fallbacks:
    - {"claude-document": ["gemini-general"]}
    - {"local-extract": ["gemini-general", "claude-document"]}

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
