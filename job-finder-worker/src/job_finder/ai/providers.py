"""AI provider abstractions for different LLM services."""

import json
import os
import subprocess
from abc import ABC, abstractmethod
from enum import Enum
from typing import Optional

from anthropic import Anthropic
from openai import OpenAI

from job_finder.exceptions import AIProviderError


class AITask(str, Enum):
    """
    Types of AI tasks with different model requirements.

    Different tasks have different cost/quality tradeoffs:
    - SCRAPE: Extract structured data from HTML (cheap, fast - Haiku)
    - FILTER: Not used (filtering is rule-based)
    - ANALYZE: Match job to profile (cheap, fast - Haiku, 95% cost savings vs Sonnet)
    - SOURCE_DISCOVERY: Discover source config and field mappings (cheap, one-time - Haiku)
    """

    SCRAPE = "scrape"
    ANALYZE = "analyze"
    SOURCE_DISCOVERY = "source_discovery"


class ModelTier(str, Enum):
    """
    Model performance tiers.

    - FAST: Cheap, fast models for simple extraction (Haiku, GPT-4o-mini)
    - SMART: Expensive, capable models for complex analysis (Sonnet, GPT-4)
    """

    FAST = "fast"
    SMART = "smart"


# Model mappings by provider and tier
MODEL_SELECTION = {
    "claude": {
        ModelTier.FAST: "claude-3-5-haiku-20241022",  # $0.001/1K tokens
        ModelTier.SMART: "claude-3-5-sonnet-20241022",  # $0.015-0.075/1K tokens
    },
    "openai": {
        ModelTier.FAST: "gpt-4o-mini",  # $0.00015-0.0006/1K tokens
        ModelTier.SMART: "gpt-4o",  # $0.0025-0.01/1K tokens
    },
}

# Task to tier mapping
TASK_MODEL_TIERS = {
    AITask.SCRAPE: ModelTier.FAST,
    AITask.ANALYZE: ModelTier.FAST,  # Changed to FAST - Haiku handles job analysis well at 95% cost savings
    AITask.SOURCE_DISCOVERY: ModelTier.FAST,
}


class AIProvider(ABC):
    """Abstract base class for AI providers."""

    @abstractmethod
    def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """
        Generate a response from the AI model.

        Args:
            prompt: The prompt to send to the model.
            max_tokens: Maximum tokens in the response.
            temperature: Sampling temperature (0.0 to 1.0).

        Returns:
            The generated text response.
        """
        pass


class ClaudeProvider(AIProvider):
    """Anthropic Claude provider."""

    def __init__(self, api_key: Optional[str] = None, model: str = "claude-opus-4-20250514"):
        """
        Initialize Claude provider.

        Args:
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var).
            model: Model identifier (default: claude-opus-4-20250514, the most capable model).
        """
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise AIProviderError(
                "Anthropic API key must be provided or set in ANTHROPIC_API_KEY environment variable"
            )

        self.model = model
        self.client = Anthropic(api_key=self.api_key)

    def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate a response using Claude."""
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.content[0].text
        except Exception as e:
            raise AIProviderError(f"Claude API error: {str(e)}") from e


class OpenAIProvider(AIProvider):
    """OpenAI GPT provider."""

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o"):
        """
        Initialize OpenAI provider.

        Args:
            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var).
            model: Model identifier.
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise AIProviderError(
                "OpenAI API key must be provided or set in OPENAI_API_KEY environment variable"
            )

        self.model = model
        self.client = OpenAI(api_key=self.api_key)

    def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate a response using GPT."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.choices[0].message.content or ""
        except Exception as e:
            raise AIProviderError(f"OpenAI API error: {str(e)}") from e


class CodexCLIProvider(AIProvider):
    """
    Codex CLI provider: uses the `codex` CLI (pro account session) instead of per-request API keys.

    Requires the `codex` binary on PATH and that the user is already authenticated (e.g., via the
    same credential copy flow used by the backend).
    """

    def __init__(self, model: str = "gpt-4o-mini", timeout: int = 60):
        self.model = model or os.getenv("CODEX_CLI_MODEL", "gpt-4o-mini")
        self.timeout = timeout

    def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        body = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant for job processing."},
                {"role": "user", "content": prompt},
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }

        try:
            result = subprocess.run(
                [
                    "codex",
                    "api",
                    "chat/completions",
                    "-m",
                    self.model,
                    "-d",
                    json.dumps(body),
                ],
                capture_output=True,
                text=True,
                timeout=self.timeout,
            )

            if result.returncode != 0:
                raise AIProviderError(
                    f"Codex CLI failed (exit {result.returncode}): {result.stderr.strip()}"
                )

            parsed = json.loads(result.stdout)
            content = parsed.get("choices", [{}])[0].get("message", {}).get("content", "")
            if not content:
                raise AIProviderError("Codex CLI returned empty content")
            return content

        except subprocess.TimeoutExpired as exc:
            raise AIProviderError(f"Codex CLI timed out after {self.timeout}s") from exc
        except json.JSONDecodeError as exc:
            raise AIProviderError("Failed to parse Codex CLI JSON response") from exc


def get_model_for_task(provider_type: str, task: AITask) -> str:
    """
    Get the appropriate model for a specific task.

    Uses cost-optimized model selection:
    - Fast/cheap models (Haiku, GPT-4o-mini) for ALL tasks (95% cost savings)
    - Haiku handles job analysis well despite being cheaper than Sonnet

    Args:
        provider_type: Type of provider ('claude', 'openai')
        task: The AI task to perform

    Returns:
        Model identifier string

    Raises:
        ValueError: If provider_type is not supported

    Example:
        >>> get_model_for_task("claude", AITask.SCRAPE)
        'claude-3-5-haiku-20241022'
        >>> get_model_for_task("claude", AITask.ANALYZE)
        'claude-3-5-haiku-20241022'
    """
    provider_type = provider_type.lower()

    if provider_type not in MODEL_SELECTION:
        raise AIProviderError(
            f"Unsupported AI provider: {provider_type}. Supported providers: {list(MODEL_SELECTION.keys())}"
        )

    tier = TASK_MODEL_TIERS[task]
    return MODEL_SELECTION[provider_type][tier]


def create_provider(
    provider_type: str,
    api_key: Optional[str] = None,
    model: Optional[str] = None,
    task: Optional[AITask] = None,
) -> AIProvider:
    """
    Factory function to create AI provider instances.

    Args:
        provider_type: Type of provider ('claude', 'openai').
        api_key: Optional API key (otherwise uses environment variable).
        model: Optional explicit model name (overrides task-based selection).
        task: Optional task type for automatic model selection (ignored if model is provided).

    Returns:
        AIProvider instance.

    Raises:
        ValueError: If provider_type is not supported.

    Example:
        # Automatic model selection for scraping (uses Haiku)
        provider = create_provider("claude", task=AITask.SCRAPE)

        # Automatic model selection for analysis (uses Sonnet)
        provider = create_provider("claude", task=AITask.ANALYZE)

        # Explicit model override
        provider = create_provider("claude", model="claude-opus-4-20250514")
    """
    provider_type = provider_type.lower()
    use_codex_cli = os.getenv("USE_CODEX_CLI", "0") == "1"

    # Determine model
    if model:
        # Explicit model provided, use it
        selected_model = model
    elif task:
        # Task provided, select appropriate model
        selected_model = get_model_for_task(provider_type, task)
    else:
        # No model or task provided, use None to trigger provider default
        selected_model = None

    if use_codex_cli and provider_type in ("openai", "codex", "codex_cli"):
        # Use CLI-based Codex to leverage pro account without per-request API keys
        cli_model = selected_model or get_model_for_task("openai", AITask.ANALYZE)
        return CodexCLIProvider(model=cli_model)

    if provider_type == "claude":
        kwargs = {"api_key": api_key} if api_key else {}
        if selected_model:
            kwargs["model"] = selected_model
        return ClaudeProvider(**kwargs)

    elif provider_type == "openai":
        kwargs = {"api_key": api_key} if api_key else {}
        if selected_model:
            kwargs["model"] = selected_model
        return OpenAIProvider(**kwargs)

    else:
        raise AIProviderError(
            f"Unsupported AI provider: {provider_type}. Supported providers: claude, openai"
        )
